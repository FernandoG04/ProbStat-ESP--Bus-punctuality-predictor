{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; line-height: 2\">\n",
    "\n",
    "To estimate the variance of the error term, we use the assumptions mentioned in the previous section. We assume that the error terms are all independent and normally distributed random variables with mean zero and variance $\\sigma^2$. From the definition of covariance we have:\n",
    "\n",
    "Let\n",
    "$$\\begin{align*}\n",
    "cov(\\varepsilon_i, \\varepsilon_j) &= \\frac{(\\varepsilon_i - \\mu_{\\varepsilon_i})(\\varepsilon_j - \\mu_{\\varepsilon_j})}{n-1}, \\hspace{1cm} i \\neq j, \\hspace{1cm} i,j = 1,2,...,n \\\\\n",
    "&= \\frac{(\\varepsilon_i-0)(\\varepsilon_j-0)}{n-1} \\\\\n",
    "&= E(\\varepsilon_i\\varepsilon_j)=0\n",
    "\\end{align*}$$\n",
    "\n",
    "where. This holds since the error terms are independent, i.e. their covariance is zero. Therefore, we define the covariance matrix of the error terms as a diagonal matrix with the variance of the error terms $\\sigma^2=(\\varepsilon_i-0)^2$ on the diagonal and denote it by E($\\varepsilon\\varepsilon^T$):\n",
    "\n",
    "$$\\begin{align*}\n",
    "E(\\varepsilon\\varepsilon^T) &= \\begin{bmatrix} \\varepsilon_1^2 & \\varepsilon_1\\varepsilon_2 & \\cdots & \\varepsilon_1\\varepsilon_n \\\\ \\varepsilon_2\\varepsilon_1 & \\varepsilon_2^2 & \\cdots & \\varepsilon_2\\varepsilon_n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\varepsilon_n\\varepsilon_1 & \\varepsilon_n\\varepsilon_2 & \\cdots & \\varepsilon_n^2 \\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix} \\sigma^2 & 0 & \\cdots & 0 \\\\ 0 & \\sigma^2 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & \\sigma^2 \\end{bmatrix} \\\\\n",
    "&= \\sigma^2I\n",
    "\\end{align*}$$\n",
    "\n",
    "where $I$ is the $n \\times n$ identity matrix. Then we use equations $(2.1.1)$ and $(2.2.2)$ to define a new matrix $M$:\n",
    "\n",
    "$$e = y - X\\beta = y - X(X^TX)^{-1}X^Ty = My$$\n",
    "\n",
    "$$M = I - X(X^TX)^{-1}X^T$$\n",
    "\n",
    "Since $X$ is symmetric, so are $X^TX$ and $(X^TX)^{-1}$. Therefore, $M$ is symmetric as well. Additionally, $M^2 = M$. To see this, we multiply $M$ by itself:\n",
    "\n",
    "$$\\begin{align*}\n",
    "M^2 &= (I - X(X^TX)^{-1}X^T)(I - X(X^TX)^{-1}X^T) \\\\ &= I - 2X(X^TX)^{-1}X^T + X(X^TX)^{-1}X^TX(X^TX)^{-1}X^T \\\\ &= I - 2X(X^TX)^{-1}X^T + X(X^TX)^{-1}X^T \\\\ &= I - X(X^TX)^{-1}X^T \\\\ &= M\n",
    "\\end{align*}$$\n",
    "\n",
    "Then we have:\n",
    "\n",
    "$$\\begin{align*}\n",
    "e &= My \\\\ &= M(X\\beta + \\varepsilon) \\\\ &= MX\\beta + M\\varepsilon \\\\ &= M\\varepsilon \n",
    "\\end{align*}$$\n",
    "\n",
    "where $MX\\beta = 0$ since $MX = 0$. Then we evaluate the variance of $e$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "var(e) &= E(ee^T) \\\\ &= E(M\\varepsilon\\varepsilon^TM^T) \\\\ &= ME(\\varepsilon\\varepsilon^T)M^T \\\\ &= M\\sigma^2IM^T \\\\ &= \\sigma^2MM^T \\\\ &= \\sigma^2M^2 \\\\ &= \\sigma^2M\n",
    "\\end{align*}$$\n",
    "\n",
    "Considering that the trace of a matrix is equal to the sum of its diagonal elements, we have:\n",
    "\n",
    "$$\\begin{align*}\n",
    "tr(var(e)) &= tr(\\sigma^2M) \\\\ &= \\sigma^2tr(M) \\\\ &= \\sigma^2tr(I_n - X(X^TX)^{-1}X^T) \\\\ &= \\sigma^2(tr(I_n) - tr(X(X^TX)^{-1}X^T)) \\\\ &= \\sigma^2(n - tr(X(X^TX)^{-1}X^T)) \\\\ &= \\sigma^2(n - tr((X^TX)^{-1}X^TX)) \\\\ &= \\sigma^2(n - tr(I_p)) \\\\ &= \\sigma^2(n - p)\n",
    "\\end{align*}$$\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\sigma^2 &= \\frac{tr(var(e))}{n - p} \\\\ &= \\frac{tr(e^Te)}{n - p} \\\\ &= \\frac{e^Te}{n - p}\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation} \\tag{2.3.1}\n",
    "\\boxed{\\hat{\\sigma}^2 = \\frac{SS_E}{n - p}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $SS_E$ is the sum of squared errors, $n$ is the number of observations, and $p =k+1$ is the number of parameters in the model. This an unbiased estimator of the variance of the error term <sup><a href=\"#footnote4\">4</a></sup>. \n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "&nbsp;  \n",
    "&nbsp;  \n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"footnote1\"></a>1: [PennState Eberly College of Science](./5_References.ipynb#1)  \n",
    "<a name=\"footnote2\"></a>2: [Ivan T. Ivanov](./5_References.ipynb#2)  \n",
    "<a name=\"footnote3\"></a>3: [OpenStax](./5_References.ipynb#3)  \n",
    "<a name=\"footnote4\"></a>4: [Heij, De Boer; Franses, Kloep; and Van Dijk](./5_References.ipynb#4)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
