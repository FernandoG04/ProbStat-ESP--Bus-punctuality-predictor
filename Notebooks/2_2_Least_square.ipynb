{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "<a style=\"flex: 1; text-align: left;\" href=\"./2_1_Multiple_linear_regresion.ipynb\">← Previous: 2.1 Multiple Linear Regression</a>\n",
    "<a style=\"flex: 1; text-align: right;\" href=\"./2_3_Error_term.ipynb\">Next: 2.3 Estimation of the Error Term →</a>\n",
    "</div>\n",
    "\n",
    "### 2.2 Least Squares Estimation of the Parameters\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; line-height: 2\">\n",
    "\n",
    "The Least Squares method is used to estimate the parameters $\\beta_i$ in the linear regression model $(1)$ using training data under the following constraint:\n",
    "\n",
    "$$\\{x_{i1}, x_{i2}, ..., x_{ik}, y_i\\}, \\hspace{.5cm} i = 1, 2, ..., n, \\hspace{.5cm}  n > k$$\n",
    "\n",
    "Where $i$ is the number of observations and $k$ is the number of variables. Thus, $x_{ij}$ is the $j$ th variable for the $i$ th observation and $y_i$ is the response variable for the $i$ th observation. The model takes this data and attempts to draw a line, or hyperplane in the multple regression case, of best fit through the data, i.e. a line or hyperplane with minimal variation of the data points from itself<sup><a href=\"#footnote3\">3</a></sup>. These variations are called residuals and are denoted by $\\epsilon_i$<sup><a href=\"#footnote3\">3</a></sup>:\n",
    "\n",
    "$$\\epsilon_i = y_i - \\hat{y}_i$$\n",
    "\n",
    "Where $\\hat{y}_i$ is the predicted value of $y_i$ based on the linear regression model $(1)$<sup><a href=\"#footnote2\">2</a></sup>:\n",
    "\n",
    "$$\\hat{y}_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_k x_{ik}$$\n",
    "\n",
    "Then, $$L=\\sum_{i=1}^{k} \\epsilon_i^2 = \\sum_{i=1}^{k} (y_i - \\beta_0 - \\sum_{j=1}^{k} \\beta_j x_{ij})^2$$ is the sum of the squared 'errors' (SSE). The Least Squares method minimizes $L$ with respect to coefficients $\\beta_0, \\beta_j$ by taking the first partial derivatives of $L$ with respect to the coefficients and setting them equal to zero <sup><a href=\"#footnote2\">2</a></sup>:\n",
    "\n",
    "$$\\begin{equation} \\tag{2} \n",
    "\\frac{\\partial L}{\\partial \\beta_j} = -2 \\sum_{i=1}^{n} (y_i - \\beta_0 - \\sum_{j=1}^{k} \\beta_j x_{ij})x_{ij} = 0\n",
    "\\end{equation}$$\n",
    "\n",
    "To solve for $\\beta_i$, we rewrite the components of equation $(2)$ as matrices<sup><a href=\"#footnote4\">4</a></sup>:\n",
    "\n",
    "$$y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\hspace{.5cm} X = \\begin{bmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & x_{n2} & \\cdots & x_{nk} \\end{bmatrix}, \\hspace{.5cm} \\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix}, \\hspace{.5cm} \\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}$$\n",
    "\n",
    "Then, the equation becomes:\n",
    "\n",
    "$$ -2X^Ty + 2X^TX\\beta = 0 $$\n",
    "\n",
    "Solving for $\\beta$ we obtain the solutions for equation $(2)$:\n",
    "$$\\begin{equation} \\tag{3}\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^Ty\n",
    "\\end{equation}$$\n",
    "given that $X^TX$ is invertible. As $X$ is a $n \\times (k+1)$ matrix, to achieve this constraint we take a sample with $n > k$ to ensure that $X^TX$ is symmetric ($n \\times n$)<sup><a href=\"#footnote4\">4</a></sup>. $\\hat{\\beta}$ is therefore the vector of coefficients estimated from the training data that minimize the SSE. The predicted value of $y_i$ is then given by the matrix notation of equation $(1)$:\n",
    "\n",
    "$$\\hat{y} = X\\hat{\\beta} +\\epsilon$$\n",
    "\n",
    "Using a bivariate example, we can see how the Least Squares method works:\n",
    "\n",
    "\n",
    "![Figure 2.2.1](../Images/2_2_1.png)\n",
    "\n",
    "<center>\n",
    "    <img src=\"../Images/2_2_1.png\" />\n",
    "    <br>\n",
    "    <em>Figure 2.2.1: Linear regression with two variables. </em>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Figure 2.2.1 shows how the regresson line is drawn through the data points. The line is drawn such that we reduce the sum of the squares of the residuals, which can be seen in green as the vertical lines from the data points to the predicted values.\n",
    "</div>\n",
    "&nbsp;  \n",
    "&nbsp;  \n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"footnote1\"></a>1: [PennState Eberly College of Science](./5_References.ipynb#1)  \n",
    "<a name=\"footnote2\"></a>2: [Ivan T. Ivanov](./5_References.ipynb#2)  \n",
    "<a name=\"footnote3\"></a>3: [OpenStax](./5_References.ipynb#3)  \n",
    "<a name=\"footnote4\"></a>4: [Heij, De Boer; Franses, Kloek; and Van Dijk](./5_References.ipynb#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "<a style=\"flex: 1; text-align: left;\" href=\"./2_1_Multiple_linear_regresion.ipynb\">← Previous: 2.1 Multiple Linear Regression</a>\n",
    "<span style=\"flex: 1; text-align: center;\">2.2 Least Squares Estimation of the Parameters</span>\n",
    "<a style=\"flex: 1; text-align: right;\" href=\"./2_3_Error_term.ipynb\">Next: 2.3 Estimation of the Error Term →</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
